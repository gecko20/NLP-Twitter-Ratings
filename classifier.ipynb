{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import twint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import Cistem\n",
    "#from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#import re\n",
    "import regex as re\n",
    "import emoji\n",
    "import datetime\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# Workaround for Jupyter's \"This event loop is already running\" exception\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if tf.test.gpu_device_name():\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(\"GPU available.\")\n",
    "else:\n",
    "    print(\"GPU not available.\")\n",
    "\n",
    "# Print numpy version\n",
    "print(np.__version__) ## For LSTM layers to work, numpy must be version 1.19.5 for some reason\n",
    "\n",
    "tweets = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#handle = \"gecko203\"\n",
    "#handle = \"FiatPecunia\"\n",
    "handle = \"Erdayastronaut\"\n",
    "batch_size = 32 #16 #4 #= 32\n",
    "buffer_size = 10000\n",
    "seed = 42\n",
    "window_size = 4\n",
    "embedding_dim = 0 #128 # Must be equal to the dimension of the trained embedding; is set later on\n",
    "num_ns = 4 # Number of negative samples\n",
    "sequence_length = 20 #10 TODO: Set to the longest tweet!\n",
    "\n",
    "tweets_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    \"{}/preproc/dataset.csv\".format(handle), batch_size=batch_size,\n",
    "    label_name=\"bin\", select_columns=['bin', 'tweet'],\n",
    "    num_epochs=1)\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    \"{}/preproc/train.csv\".format(handle), batch_size=batch_size,\n",
    "    label_name=\"bin\", select_columns=['bin', 'tweet'],\n",
    "    num_epochs=1)\n",
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    \"{}/preproc/test.csv\".format(handle), batch_size=batch_size,\n",
    "    label_name=\"bin\", select_columns=['bin', 'tweet'],\n",
    "    num_epochs=1)\n",
    "val_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    \"{}/preproc/val.csv\".format(handle), batch_size=batch_size,\n",
    "    label_name=\"bin\", select_columns=['bin', 'tweet'],\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for feature_batch, label in train_dataset.take(1):\n",
    "    for key, value in feature_batch.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "    print()\n",
    "    print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Word2Vec-Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_df = pd.read_pickle('{}/preproc/gensim_w2v_dict.pkl'.format(handle))\n",
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(emb_df[\"word\"]))\n",
    "\n",
    "# Since the StringLookup-Layer or the TextVectorization-Layer prepend two new words ('' and [UNK]),\n",
    "# we need to \"shift\" the weight matrix by two by prepending \"empty\" rows...\n",
    "# This could lead to problems if we later try to infer on new strings containing\n",
    "# words which are out of vocabulary... (?)\n",
    "\n",
    "embedding_matrix = emb_df[\"vec\"]\n",
    "embedding_dim = len(embedding_matrix[0])\n",
    "print(embedding_dim)\n",
    "\n",
    "data = []\n",
    "print(type(emb_df[\"vec\"][0]))\n",
    "data.insert(0, np.zeros(embedding_dim))\n",
    "data.insert(0, np.zeros(embedding_dim))\n",
    "embedding_matrix = pd.concat([pd.Series(data), embedding_matrix], ignore_index=True)\n",
    "print(embedding_matrix.head())\n",
    "\n",
    "\n",
    "#emb_df = pd.concat([pd.DataFrame(data), emb_df], ignore_index=True)\n",
    "#print(emb_df.head())\n",
    "#layer = StringLookup(vocabulary=list(emb_df[\"word\"]))\n",
    "vectorize_layer = TextVectorization(output_sequence_length=sequence_length,\n",
    "                                    vocabulary=list(emb_df[\"word\"]))\n",
    "\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "print(vocab_size)\n",
    "print(vectorize_layer.get_vocabulary()[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = tf.constant([\"frau man honclbrif mal\"])\n",
    "vectorize_layer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(len(inverse_vocab))\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = text['tweet']\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Map all strings to their integer representations\n",
    "train_dataset = train_dataset.map(vectorize_text)\n",
    "test_dataset  = test_dataset.map(vectorize_text)\n",
    "val_dataset   = val_dataset.map(vectorize_text)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "tweets_dataset = tweets_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally define and train our classification model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_basic(sl=20):\n",
    "    \"\"\"\n",
    "    A basic and simple sequential model.\n",
    "    :param sl: Input sequence length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Functional API\n",
    "    inputs = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    emb    = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=True,\n",
    "                              mask_zero=True,\n",
    "                              name=\"embedding\")\n",
    "    x = inputs\n",
    "    x = emb(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"basic_classifier\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_basic2(sl=20):\n",
    "    \"\"\"\n",
    "    Just the word2vec embedding and a single output neuron...\n",
    "    :param sl: Input sequence length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Functional API\n",
    "    input1 = tf.keras.Input(shape=(sl,), dtype=tf.string, name=\"tweet\")\n",
    "    # Vectorization layer not needed anymore because\n",
    "    vec1   = input1\n",
    "    emb1   = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              # Weights should be initialized after defining the model\n",
    "                              # due to protobuf's limit of 2GB:\n",
    "                              # ValueError: Message tensorflow.SavedModel exceeds maximum protobuf size of 2GB: 6768286642\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=False,\n",
    "                              name=\"embedding\",\n",
    "                              mask_zero=True)(vec1)\n",
    "    x = layers.Flatten()(emb1)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input1, outputs=outputs, name=\"embedding_classifier\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_feedforward_lstm(sl=20, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    input = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    emb1   = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              # Weights should be initialized after defining the model\n",
    "                              # due to protobuf's limit of 2GB:\n",
    "                              # ValueError: Message tensorflow.SavedModel exceeds maximum protobuf size of 2GB: 6768286642\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=True,\n",
    "                              name=\"embedding1\",\n",
    "                              mask_zero=True)(input)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(sequence_length))(emb1)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    ##x = layers.Flatten()(x)\n",
    "    ##x = layers.Dense(512, activation='relu', kernel_constraint=max_norm(3))(x)\n",
    "    ##x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=outputs, name=\"feedforward_lstm\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_stacked_lstm(sl=20, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    input = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    emb1   = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              # Weights should be initialized after defining the model\n",
    "                              # due to protobuf's limit of 2GB:\n",
    "                              # ValueError: Message tensorflow.SavedModel exceeds maximum protobuf size of 2GB: 6768286642\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=True,\n",
    "                              name=\"embedding1\",\n",
    "                              mask_zero=True)(input)\n",
    "\n",
    "    # Stacked LSTM\n",
    "    x = layers.Dropout(0.2)(emb1)\n",
    "    x = layers.Bidirectional((layers.LSTM(int(embedding_dim / 4), return_sequences=True)))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.LSTM(int(embedding_dim / 2), return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.LSTM(int(embedding_dim / 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=outputs, name=\"feedforward_lstm\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_multichannel_cnn(sl=20, output_bias=None):\n",
    "    # Bias etc. from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    # Multichannel CNN (from https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/)\n",
    "    # 1.  Embedding: Representation of words and their similarity\n",
    "    # 1.1 (LSTM directly connected to 3.?)\n",
    "    # 2.  Convolutional Model: Feature extraction\n",
    "    # 2.1 (LSTM?)\n",
    "    # 3.  Fully Connected Model: Interpretation\n",
    "\n",
    "    # Channel1\n",
    "    input1 = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    vec1   = input1\n",
    "    emb1   = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              # Weights should be initialized after defining the model\n",
    "                              # due to protobuf's limit of 2GB:\n",
    "                              # ValueError: Message tensorflow.SavedModel exceeds maximum protobuf size of 2GB: 6768286642\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=True,\n",
    "                              name=\"embedding1\",\n",
    "                              mask_zero=True)(vec1)\n",
    "    conv1 = layers.Conv1D(filters=16, kernel_size=3, activation='relu')(emb1)\n",
    "    drop1 = layers.Dropout(0.5)(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "    lstm1 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool1)\n",
    "    flat1 = layers.Flatten()(lstm1)\n",
    "\n",
    "    # Channel2\n",
    "    conv2 = layers.Conv1D(filters=16, kernel_size=6, activation='relu')(emb1)\n",
    "    drop2 = layers.Dropout(0.5)(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "    lstm2 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool2)\n",
    "    flat2 = layers.Flatten()(lstm2)\n",
    "\n",
    "    # Channel 3\n",
    "    conv3 = layers.Conv1D(filters=16, kernel_size=8, activation='relu')(emb1)\n",
    "    drop3 = layers.Dropout(0.5)(conv3)\n",
    "    pool3 = layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "    lstm3 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool3)\n",
    "    flat3 = layers.Flatten()(lstm3)\n",
    "\n",
    "\n",
    "    # Merge\n",
    "    merged = layers.concatenate([flat1, flat2, flat3])\n",
    "\n",
    "    # Interpretation\n",
    "    dense1  = layers.Dense(256, activation='relu')(merged)\n",
    "    dense1  = layers.Dense(10, activation='relu')(dense1)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(dense1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs, name=\"Classificator\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_multichannel_cnn_nogensim(sl=20, output_bias=None):\n",
    "    # Bias etc. from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    # Channel1\n",
    "    input1 = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    vec1   = input1\n",
    "    emb1   = layers.Embedding(vocab_size-0, 100)(vec1)\n",
    "    conv1 = layers.Conv1D(filters=16, kernel_size=3, activation='relu')(emb1)\n",
    "    drop1 = layers.Dropout(0.5)(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "    lstm1 = layers.Bidirectional(layers.LSTM(20, dropout=0.2, recurrent_dropout=0.0))(pool1)\n",
    "    flat1 = layers.Flatten()(lstm1)\n",
    "\n",
    "    # Channel2\n",
    "    emb2  = layers.Embedding(vocab_size-0, 100)(vec1)\n",
    "    conv2 = layers.Conv1D(filters=16, kernel_size=6, activation='relu')(emb2)\n",
    "    drop2 = layers.Dropout(0.5)(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "    lstm2 = layers.Bidirectional(layers.LSTM(20, dropout=0.2, recurrent_dropout=0.0))(pool2)\n",
    "    flat2 = layers.Flatten()(lstm2)\n",
    "\n",
    "    # Channel 3\n",
    "    emb3  = layers.Embedding(vocab_size-0, 100)(vec1)\n",
    "    conv3 = layers.Conv1D(filters=16, kernel_size=8, activation='relu')(emb3)\n",
    "    drop3 = layers.Dropout(0.5)(conv3)\n",
    "    pool3 = layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "    lstm3 = layers.Bidirectional(layers.LSTM(20, dropout=0.2, recurrent_dropout=0.0))(pool3)\n",
    "    flat3 = layers.Flatten()(lstm3)\n",
    "\n",
    "    # Merge\n",
    "    merged = layers.concatenate([flat1, flat2, flat3])\n",
    "\n",
    "    # Interpretation\n",
    "    dense1  = layers.Dense(256, activation='relu')(merged)\n",
    "    dense1  = layers.Dropout(0.2)(dense1)\n",
    "    dense1  = layers.Dense(16, activation='relu')(dense1)\n",
    "    dense1  = layers.Dropout(0.2)(dense1)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(dense1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs, name=\"Classificator\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#def tweedie_loglikelihood(y, y_hat):\n",
    "#    \"\"\"\n",
    "#    Implements the Tweedie loss function which is better suited to extremely skewed data like ours.\n",
    "#\n",
    "#    From: https://towardsdatascience.com/tweedie-loss-function-for-right-skewed-data-2c5ca470678f\n",
    "#    \"\"\"\n",
    "#    p = 2 # power hyper-parameter\n",
    "#\n",
    "#    loss = - y * tf.pow(y_hat, 1 - p) / (1 - p) + \\\n",
    "#            tf.pow(y_hat, 2 - p) / (2 - p)\n",
    "#    return tf.reduce_mean(loss)\n",
    "\n",
    "def tweedieloss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    An alternative loss function which should handle datasets with outliers better...\n",
    "    It did not help with our problem.\n",
    "    From: https://datascience.stackexchange.com/a/55393\n",
    "    \"\"\"\n",
    "    p=0.0 #1.5\n",
    "    dev = 2 * (tf.pow(y_true, 2-p)/((1-p) * (2-p)) -\n",
    "                   y_true * tf.pow(y_pred, 1-p)/(1-p) +\n",
    "                   tf.pow(y_pred, 2-p)/(2-p))\n",
    "    return tf.reduce_mean(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### This model did not work - it predicted constant values for every input!\n",
    "### -> The reason we need to first classify the input and then perform regression depending on its predicted class.\n",
    "\n",
    "#model = Sequential([\n",
    "#    layers.Input(shape=(1,), dtype=tf.string),\n",
    "#    vectorize_layer,\n",
    "#    #layers.Embedding(vocab_size, embedding_dim,\n",
    "#    #                 name=\"embedding\"),\n",
    "#    layers.Embedding(vocab_size-1, embedding_dim,\n",
    "#                     embeddings_initializer=Constant(embedding_vectors),\n",
    "#                     trainable=False,\n",
    "#                     name=\"embedding\"),\n",
    "#    #layers.GlobalAvgPool1D(),\n",
    "#    #layers.Bidirectional(layers.LSTM(512)),\n",
    "#    #layers.LSTM(64),\n",
    "#    #layers.Bidirectional(layers.Dense(64)),\n",
    "#    layers.Dense(64, activation='relu'),\n",
    "#    #layers.Dense(64, activation='relu'),\n",
    "#    layers.Dense(1)\n",
    "#])\n",
    "\n",
    "#model.compile(loss='mean_absolute_error',\n",
    "#              optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "\n",
    "epochs = 20 #100\n",
    "patience = int(epochs * 0.50) #int(epochs * 0.3)\n",
    "learning_rate = 1e-2 * 5 #1e-4 * 5\n",
    "decay_rate = learning_rate / (epochs*epochs)\n",
    "momentum = 0.8\n",
    "\n",
    "# The threshold for deciding whether prediction values are 1 or 0 should be\n",
    "# the value where we have split the data into both bins.\n",
    "#x_split:\n",
    "#threshold = 0.2754579724298949\n",
    "#threshold = 0.3\n",
    "threshold = 0.25553209091655876 # Erdayastronaut\n",
    "#threshold = 0.5\n",
    "\n",
    "#initial_bias = [0.53100804]\n",
    "initial_bias = [2.89037479] # Erdayastronaut\n",
    "\n",
    "model = make_multichannel_cnn(sl=sequence_length, output_bias=initial_bias)\n",
    "#model = make_multichannel_cnn_nogensim(sl=sequence_length, output_bias=initial_bias)\n",
    "\n",
    "\n",
    "# Compute class weights so that the optimizer doesn't get stuck\n",
    "# in a local minimum and both classes are balanced.\n",
    "labels = np.concatenate([y for x, y in train_dataset], axis=0)\n",
    "classWeights = compute_class_weight('balanced', np.unique(labels), labels)\n",
    "classWeights = dict(enumerate(classWeights))\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
    "    tf.metrics.BinaryAccuracy(threshold=threshold)\n",
    "]\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#opt = tf.keras.optimizers.SGD(lr=learning_rate, decay=decay_rate, momentum=momentum, nesterov=True) #1e-5\n",
    "#opt = tf.keras.optimizers.Adamax(learning_rate=1e-04, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              #optimizer='adam', #opt,\n",
    "              optimizer=opt,\n",
    "              #metrics=tf.metrics.BinaryAccuracy(threshold=threshold))\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, to_file='{}.png'.format(model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "print(classWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_features = test_dataset.map(lambda x, y: x)\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.autograph.set_verbosity(0)\n",
    "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "early_stopping = EarlyStopping(monitor='val_binary_accuracy', verbose=1, patience=patience)\n",
    "#early_stopping = EarlyStopping(monitor='val_precision', mode='max', verbose=1, patience=patience)\n",
    "model_checkpoint = ModelCheckpoint('{}/models/classificator_best.tf'.format(handle),\n",
    "                                   monitor='val_binary_accuracy',\n",
    "                                   #monitor='val_precision', mode='max',\n",
    "                                   save_format='tf',\n",
    "                                   #save_format='h5',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   verbose=1)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    class_weight=classWeights,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    #steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saved_model = load_model('{}/models/classificator_best.tf'.format(handle))\n",
    "\n",
    "_, train_acc = saved_model.evaluate(train_dataset, verbose=1)\n",
    "_, test_acc  = saved_model.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "print('Train Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(hist):\n",
    "    plt.plot(hist.history['loss'], label='loss')\n",
    "    plt.plot(hist.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, .3])\n",
    "    plt.xlabel=('Epoch')\n",
    "    plt.ylabel=('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for feature_batch, label in test_dataset.take(1):\n",
    "    for key, value in feature_batch.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "    print()\n",
    "    print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs_ = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs_, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs_, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs_, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs_, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "def plot_metrics(history):\n",
    "    metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "              #plt.ylim([0, plt.ylim()[1]])\n",
    "                print(\"\")\n",
    "        elif metric == 'auc':\n",
    "              plt.ylim([0.8,1])\n",
    "        else:\n",
    "              #plt.ylim([0,1])\n",
    "            print(\"\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(\"Confusion matrix @{:.2f}\".format(p))\n",
    "    plt.ylabel(\"Actual label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    print('True Negatives: ', cm[0][0])\n",
    "    print('False Positives: ', cm[0][1])\n",
    "    print('False Negatives: ', cm[1][0])\n",
    "    print('True Positives: ', cm[1][1])\n",
    "    print('Total: ', np.sum(cm[1]))\n",
    "\n",
    "test_results = model.evaluate(\n",
    "    test_dataset\n",
    ")\n",
    "print(test_results)\n",
    "\n",
    "test_features = test_dataset.map(lambda x, y: x)\n",
    "test_labels = test_dataset.map(lambda x, y: y)\n",
    "test_predictions = model.predict(test_features)\n",
    "\n",
    "#print(test_labels)\n",
    "\n",
    "ex = test_labels.unbatch()\n",
    "ex = pd.DataFrame(ex)\n",
    "print(ex)\n",
    "print(test_predictions)\n",
    "\n",
    "train_features = train_dataset.map(lambda x, y: x)\n",
    "train_labels = train_dataset.map(lambda x, y: y)\n",
    "ex2 = train_labels.unbatch()\n",
    "ex2 = pd.DataFrame(ex2)\n",
    "train_predictions = model.predict(train_features)\n",
    "\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(ex, test_predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "lims = [0, 1]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)\n",
    "\n",
    "for name, value in zip(model.metrics_names, test_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(ex, test_predictions, p=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,20])\n",
    "    plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plot_roc(\"Train\", ex2, train_predictions, color=colors[0])\n",
    "plot_roc(\"Test\", ex, test_predictions, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}