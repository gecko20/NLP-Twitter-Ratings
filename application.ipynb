{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e8c622-00a4-4897-89fa-f1de0e54a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import twint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import Cistem\n",
    "#from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#import re\n",
    "import regex as re\n",
    "import emoji\n",
    "import datetime\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# Workaround for Jupyter's \"This event loop is already running\" exception\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f3b08-f19c-4528-8ad1-bcb8cafab63d",
   "metadata": {},
   "source": [
    "# Application\n",
    "### Or: sequential execution of the trained models on a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe294d6-dc08-416b-b0e7-2764f325a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"Erdayastronaut\"\n",
    "batch_size = 32 #16 #4 #= 32\n",
    "buffer_size = 10000\n",
    "seed = 42\n",
    "window_size = 4\n",
    "embedding_dim = 0 #128 # Must be equal to the dimension of the trained embedding; is set later on\n",
    "num_ns = 4 # Number of negative samples\n",
    "sequence_length = 20\n",
    "\n",
    "# Load the trained models\n",
    "\n",
    "# TODO: Save entire models so that we don't have to copy&paste between notebooks nor redefine and reinstantiate every time...\n",
    "\n",
    "# In this example on the Erydayastronaut-Dataset, we used the CNN-Multichannel-Classifier and both basic regressors.\n",
    "classifier = None\n",
    "regressor0 = None\n",
    "regressor1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e876c31-0ca4-4f6c-8db2-75f7adff99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everybody lov elo musk spacex\n"
     ]
    }
   ],
   "source": [
    "# The input\n",
    "tweet = \"Hello, everybody! I love Elon Musk and SpaceX.\"\n",
    "tweet_raw = tweet\n",
    "\n",
    "# Preprocess the tweet\n",
    "def preprocess(t, lang='en'):\n",
    "    \"\"\"\n",
    "    Based on the preprocessing functions in the Miner notebook.\n",
    "    \"\"\"\n",
    "    # Remove Mentions\n",
    "    mentions = r'@\\w*'\n",
    "    mentions = re.compile(mentions)\n",
    "\n",
    "    # Remove URLs\n",
    "    urls = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    urls = re.compile(urls)\n",
    "\n",
    "    # Remove emojis\n",
    "    es = map(lambda x: x, emoji.UNICODE_EMOJI['en'].keys())\n",
    "    emojis = re.compile('|'.join(re.escape(e) for e in es))\n",
    "    \n",
    "    # Remove punctuations\n",
    "    punctuations = r'[^\\w\\s]|_'\n",
    "    punctuations = re.compile(punctuations)\n",
    "\n",
    "    t = t.replace(\"#\", \"\")\n",
    "    t = mentions.sub(r'', t)\n",
    "    t = urls.sub(r'', t)\n",
    "    t = emojis.sub(r'', t)\n",
    "    t = punctuations.sub(r'', t)\n",
    "    \n",
    "    # Remove control sequences\n",
    "    # Adapted from https://stackoverflow.com/a/19016117\n",
    "    t = \"\".join(ch for ch in t if unicodedata.category(ch)[0]!=\"C\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    if lang == 'de':\n",
    "        stopwords_ = stopwords.words('german')\n",
    "        stopwords_file = open(\"stop_words_german.txt\", \"r\")\n",
    "        sws = stopwords_file.read().split(\"\\n\")\n",
    "        stopwords_file.close()\n",
    "        stopwords_ += sws\n",
    "        no_stopwords = ['mann', 'mensch', 'menschen', 'recht', 'rechte', 'rechten', 'rechter', 'rechtes']\n",
    "        additional_stopwords = ['mal', 'halt', 'sagen', 'schon', 'lassen', 'danke', 'bitte', 'einfach', 'eigentlich', 'schon', 'sich']\n",
    "        stopwords_ += additional_stopwords\n",
    "        stopwords_ = [x for x in stopwords_ if x not in no_stopwords]\n",
    "    elif lang == 'en':\n",
    "        stopwords_ = stopwords.words('english')\n",
    "    else:\n",
    "        print(\"Language not supported yet, please add it to this function...\")\n",
    "        return\n",
    "    \n",
    "    stemmer = Cistem()\n",
    "    \n",
    "    tweetClean = []\n",
    "    tokenizedTweet = tokenizer.tokenize(t)\n",
    "    for word in tokenizedTweet:\n",
    "        if (word not in stopwords_):\n",
    "            stem = stemmer.stem(word)\n",
    "            tweetClean.append(stem)\n",
    "    \n",
    "    return ' '.join(tweetClean)\n",
    "\n",
    "tweet = preprocess(tweet)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cfbd4bb-408b-438b-af7d-cf52eccae2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 998 1985   59  620  675   17    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]], shape=(1, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Convert the string to its integer representation\n",
    "emb_df = pd.read_pickle('{}/preproc/gensim_w2v_dict.pkl'.format(handle))\n",
    "# Since the StringLookup-Layer or the TextVectorization-Layer prepend two new words ('' and [UNK]),\n",
    "# we need to \"shift\" the weight matrix by two by prepending \"empty\" rows...\n",
    "# This could lead to problems if we later try to infer on new strings containing\n",
    "# words which are out of vocabulary... (?)\n",
    "\n",
    "embedding_matrix = emb_df[\"vec\"]\n",
    "embedding_dim = len(embedding_matrix[0])\n",
    "\n",
    "data = []\n",
    "data.insert(0, np.zeros(embedding_dim))\n",
    "data.insert(0, np.zeros(embedding_dim))\n",
    "embedding_matrix = pd.concat([pd.Series(data), embedding_matrix], ignore_index=True)\n",
    "\n",
    "\n",
    "#emb_df = pd.concat([pd.DataFrame(data), emb_df], ignore_index=True)\n",
    "#print(emb_df.head())\n",
    "#layer = StringLookup(vocabulary=list(emb_df[\"word\"]))\n",
    "vectorize_layer = TextVectorization(output_sequence_length=sequence_length,\n",
    "                                    vocabulary=list(emb_df[\"word\"]))\n",
    "\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = text['tweet']\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "tweet = tf.constant([tweet])\n",
    "tweet = vectorize_layer(tweet)\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590f143f-f1fb-4f83-a9e1-88d5a00a2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4348981]]\n"
     ]
    }
   ],
   "source": [
    "# Run the classificator and keep the result\n",
    "def make_multichannel_cnn(sl=20, output_bias=None):\n",
    "    # Bias etc. from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    # Multichannel CNN (from https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/)\n",
    "    # 1.  Embedding: Representation of words and their similarity\n",
    "    # 1.1 (LSTM directly connected to 3.?)\n",
    "    # 2.  Convolutional Model: Feature extraction\n",
    "    # 2.1 (LSTM?)\n",
    "    # 3.  Fully Connected Model: Interpretation\n",
    "\n",
    "    # Channel1\n",
    "    input1 = tf.keras.Input(shape=(sl,), dtype=tf.int32, name=\"tweet\")\n",
    "    vec1   = input1\n",
    "    emb1   = layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                              # Weights should be initialized after defining the model\n",
    "                              # due to protobuf's limit of 2GB:\n",
    "                              # ValueError: Message tensorflow.SavedModel exceeds maximum protobuf size of 2GB: 6768286642\n",
    "                              embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                              #input_length=batch_size, Deprecated?\n",
    "                              trainable=True,\n",
    "                              name=\"embedding1\",\n",
    "                              mask_zero=True)(vec1)\n",
    "    conv1 = layers.Conv1D(filters=16, kernel_size=3, activation='relu')(emb1)\n",
    "    drop1 = layers.Dropout(0.5)(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "    lstm1 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool1)\n",
    "    flat1 = layers.Flatten()(lstm1)\n",
    "\n",
    "    # Channel2\n",
    "    conv2 = layers.Conv1D(filters=16, kernel_size=6, activation='relu')(emb1)\n",
    "    drop2 = layers.Dropout(0.5)(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "    lstm2 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool2)\n",
    "    flat2 = layers.Flatten()(lstm2)\n",
    "\n",
    "    # Channel 3\n",
    "    conv3 = layers.Conv1D(filters=16, kernel_size=8, activation='relu')(emb1)\n",
    "    drop3 = layers.Dropout(0.5)(conv3)\n",
    "    pool3 = layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "    lstm3 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.0))(pool3)\n",
    "    flat3 = layers.Flatten()(lstm3)\n",
    "\n",
    "\n",
    "    # Merge\n",
    "    merged = layers.concatenate([flat1, flat2, flat3])\n",
    "\n",
    "    # Interpretation\n",
    "    dense1  = layers.Dense(256, activation='relu')(merged)\n",
    "    dense1  = layers.Dense(10, activation='relu')(dense1)\n",
    "    outputs = layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)(dense1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs, name=\"Classificator\")\n",
    "\n",
    "    return model\n",
    "\n",
    "classifier = make_multichannel_cnn(sl=sequence_length)\n",
    "\n",
    "# Load the weights from the last checkpoint in classifier.ipynb\n",
    "classifier.load_weights('{}/models/classificator_best.tf'.format(handle))\n",
    "\n",
    "\n",
    "result = classifier.predict(tweet)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb49635a-d818-4ee9-980d-66383aeca558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Predicted class 1... running regressor 1:\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f44dc1a1f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "\n",
      "Predicted Rating for tweet: Hello, everybody! I love Elon Musk and SpaceX.\n",
      "[[0.51202065]]\n"
     ]
    }
   ],
   "source": [
    "# Now run one of both regressors depending on the tweet's predicted class\n",
    "def make_basic(sl=20):\n",
    "    \"\"\"\n",
    "    A basic and simple sequential model.\n",
    "    :param sl: Input sequence length\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(sl,), dtype=tf.int32),\n",
    "        layers.Embedding(vocab_size-0, embedding_dim,\n",
    "                         embeddings_initializer=Constant(list(embedding_matrix)),\n",
    "                         #input_length=batch_size, Deprecated?\n",
    "                         trainable=False,\n",
    "                         mask_zero=True,\n",
    "                         name=\"embedding\"),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.LSTM(64),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "threshold = 0.25553209091655876 # Erdayastronaut (from the classifier notebook)\n",
    "\n",
    "regressor0 = make_basic(sl=sequence_length)\n",
    "regressor1 = make_basic(sl=sequence_length)\n",
    "\n",
    "regressor0.load_weights('{}/models/regressor0_best.tf'.format(handle))\n",
    "regressor1.load_weights('{}/models/regressor1_best.tf'.format(handle))\n",
    "\n",
    "rating = 0.0\n",
    "\n",
    "if result <= threshold:\n",
    "    print(\"Predicted class 0... running regressor 0:\")\n",
    "    rating = regressor0.predict(tweet)\n",
    "else:\n",
    "    print(\"Predicted class 1... running regressor 1:\")\n",
    "    rating = regressor1.predict(tweet)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Predicted Rating for tweet: {}\".format(tweet_raw))\n",
    "print(rating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
